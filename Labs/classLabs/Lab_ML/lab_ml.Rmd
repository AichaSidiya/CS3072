---
title: "Lab7_May7"
author: "Aicha"
date: "2023-05-07"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First we will import the dataset. The dataset is about two brands of orange juice.

```{r}
library(ggplot2)
library(caret)
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
str(orange)
head(orange[, 1:10])

```

Now we will split the data into training and testng for this purpose, we will use createDataPartition method.
set.seed(100) to have the random data be the same every time we run

```{r}
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
```

Before we do further data processing on the data, we can also check some stats about the dataset. The skimer package provide a good solution to do so.

```{r}
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]

```

Now, we will fill the missing values with the TrainData dataset. The most common algorithm used for this purpose is KNN. We will use preprocess and predict function to do this task.

```{r}
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model

# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnImpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)

```
It is common to have categorical variables in the dataset. In order to convert to numerical to be useful in the machine learning models, we can implemnet the one-hot-encoding using the dummyVars() as function as the following:

```{r}
dummies_model <- dummyVars(Purchase ~ ., data=trainData)

trainData_mat <- predict(dummies_model, newdata = trainData)

trainData <- data.frame(trainData_mat)

str(trainData)

```
```{r}
preProcess_range_model <- preProcess(trainData, method = 'range')
trainData <- predict(preProcess_range_model, newdata = trainData)

trainData$Purchase <- y
```

We have many machine learning models supported by caret as shown below:

```{r}
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames

```
In the following section we will train a random forset model

```{r}
modelLookup('earth')
```
```{r}
# Set the seed for reproducibility
set.seed(100)

model_mars = train(Purchase ~ ., data=trainData, method = 'earth')
fitted <- predict(model_mars)
model_mars

```
Now, we have the machine learning model (mars). We will tast this model using the test dataset that we kept earlier. First we will preprocess the test datasetas the folowing:

```{r}
# Step 1: Impute missing values 
testData2 <- predict(preProcess_missingdata_model, testData)  

# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)

# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)

# View
head(testData4[, 1:10])

```

Now we will use the trained model to analyze the tast data and provide us with prediction

```{r}
# Predict on testData
predicted <- predict(model_mars, testData4)
head(predicted)

```

Now, we will predict compare the predicted values against the actual values.

```{r}
confusionMatrix(reference = as.factor(testData$Purchase), data = predicted, mode = 'everything', positive = 'MM')
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```
For many machine learning models we can implemnt some performance tuning for the model. The perfoemance tuning model aims to have a higher accuracy for the model. It is very common to do this for any model you create.

```{r}
#performance tuning
fitControl <- trainControl(
  method = 'cv', #k - folds validation
  number = 5,
  savePredictions = 'final', #saves prediction for oprimal tuning parameters
  classProbs = T, #should pass probabilites to be returned
  summaryFunction = twoClassSummary #results
)

#step 1: Tune hyper parameters by setting tuneLength
set.seed(100)
model_mars2 = train(Purchase ~ ., data = trainData, methods = 'earth', tuneLength = 5, metrics = 'ROC', trControl = fitControl)

model_mars2

# #step2 : predict and testData and compute the confusion metrics
# predicted2 <- predict(model_mars2, testData4)
# confusionMatrix(reference = as.factor(testData$Purchase), data = predicted2, mode = 'everything', positve = 'MM')
# 
# #step3: Define the tuneGrid
# marsGrid <- expand.grid(nprune = c(2, 4, 6, 8, 10),
#                         degree = c(1,2,3))
# #model3
# set.seed(100)
# model_mars3 = train(Purchase~., data = trainData, methods = 'earth', tuneGrid = marsGrid, metrics = 'ROC', trControl = fitControl)
# model_mars3
# 
# predicted3 <- predict(model_mars3, testData4)
# confusionMatrix(reference = as.factor(testData$Purchase), data = #predicted3, mode = 'everything', positve = 'MM')
```

Finally we will train two more modles and we compare the performance of each one of them. 
After creating the models, we can compare their performance using resample function where you list all the models that you created.

```{r}
set.seed(100)

#random forest

model_rf = train(Purchase ~ ., data = trainData, method = 'rf', tuneLength = 5, trControl = fitControl)
model_rf 

set.seed(100)

#svm

model_svmRadial = train(Purchase ~ ., data = trainData, method = 'svmRadial', tuneLength = 15, trControl = fitControl)
model_svmRadial

#Compare all 3 using resample

models_compare <- resamples(list(RF = model_rf, MARS = model_mars2, SVM = model_svmRadial))

summary(models_compare)
```

